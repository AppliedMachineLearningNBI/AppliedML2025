{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ce6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time # To demonstrate performance implications of num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9b69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a Custom Dataset\n",
    "# ---------------------------\n",
    "# Your custom dataset must inherit from torch.utils.data.Dataset\n",
    "# and implement __len__ and __getitem__.\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple custom dataset that generates synthetic data.\n",
    "    In a real-world scenario, this is where you would load your actual data,\n",
    "    e.g., file paths to images, lines from a text file, rows from a CSV, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=1000, feature_dim=10, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Total number of samples in the dataset.\n",
    "            feature_dim (int): Dimensionality of each data sample.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "                                           (e.g., data augmentation for images)\n",
    "        \"\"\"\n",
    "        print(f\"Initializing MyCustomDataset with {num_samples} samples.\")\n",
    "        # For this example, we'll create some random data and labels.\n",
    "        # In a real dataset, self.data might be a list of file paths,\n",
    "        # and self.labels might be loaded from a manifest file.\n",
    "        self.data = torch.randn(num_samples, feature_dim)  # Synthetic data\n",
    "        self.labels = torch.randint(0, 2, (num_samples,)) # Synthetic binary labels (0 or 1)\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        This is crucial for the DataLoader to know how many items it can fetch.\n",
    "        \"\"\"\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample (data and label) at the given index `idx`.\n",
    "        This method is called by the DataLoader to fetch individual items.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, label) where sample is the data and label is its corresponding label.\n",
    "        \"\"\"\n",
    "        # Retrieve the raw data and label\n",
    "        sample_data = self.data[idx]\n",
    "        sample_label = self.labels[idx]\n",
    "\n",
    "        # Apply transformations if any are provided\n",
    "        # (e.g., for image datasets, this could be normalization, resizing, augmentation)\n",
    "        if self.transform:\n",
    "            sample_data = self.transform(sample_data)\n",
    "\n",
    "        # In more complex scenarios, `idx` might be used to:\n",
    "        # - Load an image from `self.file_paths[idx]`\n",
    "        # - Read a specific line from a large text file\n",
    "        # - Query a database\n",
    "        # The key is that it should return one processed data point and its label.\n",
    "        return sample_data, sample_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b713d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating Dataset Instance ---\n",
      "Initializing MyCustomDataset with 100 samples.\n",
      "Length of dataset: 100\n",
      "First sample (data at index 0): tensor([ 0.0135,  0.6356, -0.0082, -0.6313, -1.9793])\n",
      "First sample label (label at index 0): 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Instantiate the Custom Dataset\n",
    "# ---------------------------------\n",
    "print(\"\\n--- Creating Dataset Instance ---\")\n",
    "custom_dataset = MyCustomDataset(num_samples=100, feature_dim=5)\n",
    "\n",
    "# Let's test __len__ and __getitem__\n",
    "print(f\"Length of dataset: {len(custom_dataset)}\")\n",
    "sample_idx = 0\n",
    "first_sample_data, first_sample_label = custom_dataset[sample_idx]\n",
    "print(f\"First sample (data at index {sample_idx}): {first_sample_data}\")\n",
    "print(f\"First sample label (label at index {sample_idx}): {first_sample_label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9fab0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating DataLoader Instance ---\n",
      "DataLoader settings: batch_size=16, shuffle=True, num_workers=0, pin_memory=False\n"
     ]
    }
   ],
   "source": [
    "# 3. Define the DataLoader\n",
    "# ------------------------\n",
    "# The DataLoader takes a Dataset and provides an iterable over it,\n",
    "# handling batching, shuffling, and parallel data loading.\n",
    "print(\"\\n--- Creating DataLoader Instance ---\")\n",
    "\n",
    "# Important DataLoader parameters:\n",
    "# - dataset: The Dataset object from which to load the data.\n",
    "# - batch_size (int, optional, default=1): How many samples per batch to load.\n",
    "# - shuffle (bool, optional, default=False): Set to True to have the data reshuffled\n",
    "#   at every epoch (good for training to ensure batches are different each time).\n",
    "# - num_workers (int, optional, default=0): How many subprocesses to use for data\n",
    "#   loading. 0 means that the data will be loaded in the main process.\n",
    "#   Increasing this can significantly speed up data loading by leveraging multiple\n",
    "#   CPU cores, especially if __getitem__ involves I/O or heavy computation.\n",
    "#   (BEWARE: On Windows, be careful with num_workers > 0, use if __name__ == '__main__':)\n",
    "# - pin_memory (bool, optional, default=False): If True, the DataLoader will copy\n",
    "#   tensors into CUDA pinned memory before returning them. This can speed up\n",
    "#   data transfer from CPU to GPU. Typically used when training on a GPU.\n",
    "# - drop_last (bool, optional, default=False): Set to True to drop the last incomplete\n",
    "#   batch, if the dataset size is not divisible by the batch size. If False and\n",
    "#   the size of dataset is not divisible by the batch_size, then the last batch\n",
    "#   will be smaller.\n",
    "\n",
    "batch_size_val = 16 # Try changing this\n",
    "shuffle_val = True    # Try setting to False\n",
    "num_workers_val = 0   # Try 0, then 2 or 4 if you have multiple cores, to see effect\n",
    "pin_memory_val = torch.cuda.is_available() # Only pin memory if a GPU is available\n",
    "\n",
    "print(f\"DataLoader settings: batch_size={batch_size_val}, shuffle={shuffle_val}, num_workers={num_workers_val}, pin_memory={pin_memory_val}\")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=custom_dataset,\n",
    "    batch_size=batch_size_val,\n",
    "    shuffle=shuffle_val,\n",
    "    num_workers=num_workers_val,\n",
    "    pin_memory=pin_memory_val,\n",
    "    drop_last=False # Usually False for validation/test, can be True for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c879867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iterating through DataLoader (1 epoch) ---\n",
      "\n",
      "Epoch 1\n",
      "  Batch 1:\n",
      "    Data shape: torch.Size([16, 5])\n",
      "    Labels shape: torch.Size([16])\n",
      "  Batch 2:\n",
      "    Data shape: torch.Size([16, 5])\n",
      "    Labels shape: torch.Size([16])\n",
      "  Batch 3:\n",
      "    Data shape: torch.Size([16, 5])\n",
      "    Labels shape: torch.Size([16])\n",
      "Time taken for epoch with main process loading: 0.0039 seconds\n"
     ]
    }
   ],
   "source": [
    "# 4. Iterate through the DataLoader (Example Usage)\n",
    "# -------------------------------------------------\n",
    "print(\"\\n--- Iterating through DataLoader (1 epoch) ---\")\n",
    "# In a training loop, you would iterate over the data_loader for each epoch.\n",
    "start_time = time.time()\n",
    "for epoch in range(1): # Simulating one epoch\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    for batch_idx, (batch_data, batch_labels) in enumerate(data_loader):\n",
    "        # batch_data will be a tensor of shape (batch_size, feature_dim)\n",
    "        # batch_labels will be a tensor of shape (batch_size)\n",
    "\n",
    "        # If pin_memory=True and you have a GPU, data is already in pinned memory.\n",
    "        # You would typically move data to the GPU here:\n",
    "        # if torch.cuda.is_available():\n",
    "        #     batch_data = batch_data.to('cuda')\n",
    "        #     batch_labels = batch_labels.to('cuda')\n",
    "\n",
    "        if batch_idx < 3: # Print first few batches\n",
    "            print(f\"  Batch {batch_idx+1}:\")\n",
    "            print(f\"    Data shape: {batch_data.shape}\")\n",
    "            print(f\"    Labels shape: {batch_labels.shape}\")\n",
    "            # print(f\"    Sample data from batch: {batch_data[0]}\") # First item in batch\n",
    "            # print(f\"    Sample label from batch: {batch_labels[0]}\")\n",
    "\n",
    "        # Here, you would typically pass batch_data to your model, calculate loss, etc.\n",
    "        # model(batch_data)\n",
    "        # loss = criterion(output, batch_labels)\n",
    "        # ...\n",
    "\n",
    "    # You can try timing the loop with different num_workers to see the impact\n",
    "    # For this tiny synthetic dataset, the difference might be negligible or even\n",
    "    # negative due to overhead. For datasets involving disk I/O, num_workers is key.\n",
    "    if num_workers_val > 0 :\n",
    "        print(f\"Time taken for epoch with {num_workers_val} workers: {time.time() - start_time:.4f} seconds\")\n",
    "    else:\n",
    "        print(f\"Time taken for epoch with main process loading: {time.time() - start_time:.4f} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
